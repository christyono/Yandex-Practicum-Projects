{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Open the file and study the general information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             9091 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0     2.0       0.00              1          1               1   \n",
      "1     1.0   83807.86              1          0               1   \n",
      "2     8.0  159660.80              3          1               0   \n",
      "3     1.0       0.00              2          0               0   \n",
      "4     2.0  125510.82              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/datasets/Churn.csv\")\n",
    "\n",
    "print(data.info())\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first glance at our dataset shows that our tenure column has several null values. We'll make the assumption that null means 0 years of tenure.\n",
    "\n",
    "Let's try to determine the features that are useful for our machine learning model - by removing any features that aren't useful in predicting our target, we'll reduce the time it takes to train our ML model. In this case, **RowNumber, CustomerId, and Surname** are features that won't particularly help us. \n",
    "\n",
    "After removing these features, we'll convert our categorical features into numerical features. These include **Geography and Gender**.\n",
    "\n",
    "Lastly, we'll split our data into our **training, test, and validation sets**, with a 3:1:1 ratio.\n",
    "\n",
    "Our target is **exited**, with 1 specifying that the customer has stopped using our services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#fill tenure\n",
    "data['Tenure'] = data['Tenure'].fillna(0)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(3), int64(6), object(2)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#drop unnecessary columns\n",
    "ml_data = data.drop(['RowNumber','CustomerId','Surname'], axis=1)\n",
    "\n",
    "ml_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      "CreditScore          10000 non-null int64\n",
      "Age                  10000 non-null int64\n",
      "Tenure               10000 non-null float64\n",
      "Balance              10000 non-null float64\n",
      "NumOfProducts        10000 non-null int64\n",
      "HasCrCard            10000 non-null int64\n",
      "IsActiveMember       10000 non-null int64\n",
      "EstimatedSalary      10000 non-null float64\n",
      "Exited               10000 non-null int64\n",
      "Geography_Germany    10000 non-null uint8\n",
      "Geography_Spain      10000 non-null uint8\n",
      "Gender_Male          10000 non-null uint8\n",
      "dtypes: float64(3), int64(6), uint8(3)\n",
      "memory usage: 732.5 KB\n",
      "None\n",
      "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
      "0          619   42     2.0       0.00              1          1   \n",
      "1          608   41     1.0   83807.86              1          0   \n",
      "2          502   42     8.0  159660.80              3          1   \n",
      "3          699   39     1.0       0.00              2          0   \n",
      "4          850   43     2.0  125510.82              1          1   \n",
      "\n",
      "   IsActiveMember  EstimatedSalary  Exited  Geography_Germany  \\\n",
      "0               1        101348.88       1                  0   \n",
      "1               1        112542.58       0                  0   \n",
      "2               0        113931.57       1                  0   \n",
      "3               0         93826.63       0                  0   \n",
      "4               1         79084.10       0                  0   \n",
      "\n",
      "   Geography_Spain  Gender_Male  \n",
      "0                0            0  \n",
      "1                1            0  \n",
      "2                0            0  \n",
      "3                0            0  \n",
      "4                1            0  \n"
     ]
    }
   ],
   "source": [
    "#convert categorical to numerical\n",
    "data_ohe = pd.get_dummies(ml_data, drop_first=True) #the drop_first parameter removes the Geography_France and Female columns.\n",
    "\n",
    "print(data_ohe.info())\n",
    "print(data_ohe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 12) (2000, 12) (2000, 12)\n"
     ]
    }
   ],
   "source": [
    "#split 3 ways - training, validating, and test.\n",
    "df_train, df_temp = train_test_split(data_ohe, test_size=0.40, random_state=12345)\n",
    "df_valid, df_test = train_test_split(df_temp, test_size=0.50, random_state=12345)\n",
    "\n",
    "print(\n",
    "df_train.shape,\n",
    "df_valid.shape,\n",
    "df_test.shape)\n",
    "\n",
    "#features and target for training\n",
    "features_train = df_train.drop('Exited', axis=1)\n",
    "target_train = df_train['Exited']\n",
    "\n",
    "#features and target for validating\n",
    "features_valid = df_valid.drop('Exited', axis=1)\n",
    "target_valid = df_valid['Exited']\n",
    "\n",
    "#features and target for testing\n",
    "features_test = df_test.drop('Exited', axis=1)\n",
    "target_test = df_test['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've completed preprocessing the data and splitting the data into different sets, we can work with it. We'll first check the balance of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Check the balance of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion of customers that have left the bank:  0.2037\n"
     ]
    }
   ],
   "source": [
    "print(\"Portion of customers that have left the bank: \", data['Exited'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the mean shows us a significant imbalance in classes, with an approximate 1:4 ratio of ones to zeros for the 'Exited' column. Let's first create a model without regard for the imbalance, and see our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model F1 score: 0.33389544688026984\n",
      "Decision tree model F1 score: 0.4737484737484738\n",
      "Random forest model F1 score: 0.5168195718654434\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "\n",
    "#train model\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "#make prediction\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "#check F1 score\n",
    "print(\"Logistic regression model F1 score:\", f1_score(target_valid, predicted_valid))\n",
    "\n",
    "#create model\n",
    "model = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "#train model\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "#make prediction\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "#check F1 score\n",
    "print(\"Decision tree model F1 score:\", f1_score(target_valid, predicted_valid))\n",
    "\n",
    "#create model\n",
    "model = RandomForestClassifier(random_state=123, n_estimators=10)\n",
    "\n",
    "#train model\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "#make prediction\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "#check F1 score\n",
    "print(\"Random forest model F1 score:\", f1_score(target_valid, predicted_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Improve the quality of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our quick test for our F1 score shows that our model has a low F1 score. This shows that our model has both low precision, and low recall. Let's try to improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make sure our the data in our model is scaled - this way, the different columns aren't weighed differently due to varied dispersions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CreditScore       Age    Tenure   Balance  NumOfProducts  HasCrCard  \\\n",
      "7479    -0.886751 -0.373192  1.104696  1.232271      -0.891560          1   \n",
      "3411     0.608663 -0.183385  1.104696  0.600563      -0.891560          0   \n",
      "6027     2.052152  0.480939 -0.503694  1.027098       0.830152          0   \n",
      "1247    -1.457915 -1.417129  0.461340 -1.233163       0.830152          1   \n",
      "3716     0.130961 -1.132419 -0.825373  1.140475      -0.891560          0   \n",
      "\n",
      "      IsActiveMember  EstimatedSalary  Geography_Germany  Geography_Spain  \\\n",
      "7479               0        -0.187705                  0                1   \n",
      "3411               0        -0.333945                  0                0   \n",
      "6027               1         1.503095                  1                0   \n",
      "1247               0        -1.071061                  0                0   \n",
      "3716               0         1.524268                  1                0   \n",
      "\n",
      "      Gender_Male  \n",
      "7479            1  \n",
      "3411            0  \n",
      "6027            1  \n",
      "1247            1  \n",
      "3716            0  \n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "numeric = ['CreditScore', 'Age', 'Tenure', 'Balance','NumOfProducts','EstimatedSalary']\n",
    "scaler.fit(features_train[numeric])\n",
    "\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "\n",
    "print(features_train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weight adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our F1 score by adjusting the weight of our classes in our model. We'll make it so that class \"1\" weighs more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4888888888888888\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear', class_weight='balanced') #specifying the class_weight parameter allows us to adjust our class weight\n",
    "\n",
    "#train model\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "#make prediction\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "#check F1 score\n",
    "print(\"F1:\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing so, our F1 score has already significantly improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling/Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a method to downsample our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def method for downsampling, which takes a set of features and target, and fraction of data to return\n",
    "def downsample(features, target, fraction):\n",
    "    #split training sample into negative and positive observations\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    #randomly remove negative class observations to fraction specified in parameter, and combine them with the positive class observations\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    #do the same for the target\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    \n",
    "    #shuffle data to improve training\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "    \n",
    "    #return the upsampled features and target\n",
    "    return features_downsampled, target_downsampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a method to upsample our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def method for upsampling, which takes a set of features and target, and number of repetitions\n",
    "def upsample(features, target, repeat):\n",
    "    #split training sample into negative and positive observations\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    #duplicate positive class observations, and combine them with the negative class observations\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    #do the same for the target\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    #shuffle data to improve training\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    #return the upsampled features and target\n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our data, combining different fractions for downsampling and number of repeats for upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling with fraction of 0.1 :\n",
      "F1: 0.42986425339366513\n",
      "Downsampling with fraction of 0.2 :\n",
      "F1: 0.4791344667697063\n",
      "Downsampling with fraction of 0.3 :\n",
      "F1: 0.4985835694050992\n",
      "Downsampling with fraction of 0.4 :\n",
      "F1: 0.5054229934924078\n",
      "Downsampling with fraction of 0.5 :\n",
      "F1: 0.4664224664224664\n",
      "Downsampling with fraction of 0.6 :\n",
      "F1: 0.42219215155615697\n",
      "Downsampling with fraction of 0.7 :\n",
      "F1: 0.3883211678832117\n",
      "Downsampling with fraction of 0.8 :\n",
      "F1: 0.365891472868217\n",
      "Downsampling with fraction of 0.9 :\n",
      "F1: 0.36038961038961037\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1, 1.0, 0.1):\n",
    "    features_train_downsampled, target_train_downsampled = downsample(features_train, target_train, i)\n",
    "    #create model\n",
    "    model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "    #train model\n",
    "    model.fit(features_train_downsampled, target_train_downsampled)\n",
    "\n",
    "    #make prediction\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "\n",
    "    #check F1 score\n",
    "    print(\"Downsampling with fraction of\", round(i, 1), \":\\n\"\"F1:\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampling with 2 repeats:\n",
      "F1: 0.4672435105067985\n",
      "Upsampling with 3 repeats:\n",
      "F1: 0.5014985014985014\n",
      "Upsampling with 4 repeats:\n",
      "F1: 0.4888888888888888\n",
      "Upsampling with 5 repeats:\n",
      "F1: 0.48433919022154315\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 6):\n",
    "    features_train_upsampled, target_train_upsampled = upsample(features_train, target_train, i)\n",
    "    #create model\n",
    "    model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "\n",
    "    #train model\n",
    "    model.fit(features_train_upsampled, target_train_upsampled)\n",
    "\n",
    "    #make prediction\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "\n",
    "    #check F1 score\n",
    "    print(\"Upsampling with\", i, \"repeats:\\n\"\"F1:\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've tested our model using logistic regression, along with upsampling and downsampling separately. As we can see, **downsampling with a fraction of .4 and upsampling by multiplying our positive class observations by 3** provides the greatest increase of our F1 score by **0.5054229934924078 and 0.5014985014985014**, respectively. This is already an improvement from using the balancer! We'll try to use upsampling and downsampling together to see, and observe our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's inneficient to call two methods to upsample and downsample, we'll create a function called **create_sample** that combines upsampling and downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(features, target, fraction, repeat):\n",
    "    #split training sample into negative and positive observations\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    #randomly remove negative class observations to fraction specified in parameter, and combine them with the positive class observations\n",
    "    features_sampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones] * repeat)\n",
    "    #do the same for the target\n",
    "    target_sampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones] * repeat)\n",
    "    \n",
    "    #shuffle data to improve training\n",
    "    features_sampled, target_sampled = shuffle(\n",
    "        features_sampled, target_sampled, random_state=12345)\n",
    "    \n",
    "    #return the upsampled features and target\n",
    "    return features_sampled, target_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function called **find_best_f1** so we can test different models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function takes features, target, a model, and a toggle for showing all calculations.\n",
    "def find_best_f1(features, target, model, show_all):\n",
    "    maxf1 = 0\n",
    "    frac_repeat = ()\n",
    "    for fraction in np.arange(0,1, 0.1):\n",
    "        if fraction > 0:\n",
    "            features_sampled, target_sampled = downsample(features, target, fraction)\n",
    "            #train model\n",
    "            model.fit(features_sampled, target_sampled)\n",
    "\n",
    "            #make prediction\n",
    "            predicted_valid = model.predict(features_valid)\n",
    "            \n",
    "            #check F1 score\n",
    "            f1 = f1_score(target_valid, predicted_valid)\n",
    "            frac = round(fraction, 1)\n",
    "            if(show_all):\n",
    "                print(\"sample with fraction of\", frac, \"and 0 repeats:\\nF1:\", f1)\n",
    "            if(f1>maxf1):\n",
    "                maxf1 = f1\n",
    "                frac_repeat = (frac, 0)\n",
    "        \n",
    "        for repeat in range(2,6):\n",
    "            if fraction==0:\n",
    "                features_sampled, target_sampled = upsample(features, target, repeat)\n",
    "            else:\n",
    "                features_sampled, target_sampled = create_sample(features, target, fraction, repeat)\n",
    "\n",
    "            #train model\n",
    "            model.fit(features_sampled, target_sampled)\n",
    "\n",
    "            #make prediction\n",
    "            predicted_valid = model.predict(features_valid)\n",
    "\n",
    "            #check F1 score\n",
    "            f1 = f1_score(target_valid, predicted_valid)\n",
    "            frac = round(fraction, 1)\n",
    "            if(show_all):\n",
    "                print(\"sample with fraction of\", frac, \"and\", repeat, \"repeats:\\nF1:\", f1)\n",
    "            if(f1>maxf1):\n",
    "                maxf1 = f1\n",
    "                frac_repeat = (frac, repeat)\n",
    "    \n",
    "    print(\"Max f1 score =\", maxf1, \"\\nDownsample fraction:\", frac_repeat[0], \"Upsample repetitions:\", frac_repeat[1])\n",
    "    return maxf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to find the best ratios for upsampling and downsampling for our given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample with fraction of 0.0 and 2 repeats:\n",
      "F1: 0.4672435105067985\n",
      "sample with fraction of 0.0 and 3 repeats:\n",
      "F1: 0.5014985014985014\n",
      "sample with fraction of 0.0 and 4 repeats:\n",
      "F1: 0.4888888888888888\n",
      "sample with fraction of 0.0 and 5 repeats:\n",
      "F1: 0.48433919022154315\n",
      "sample with fraction of 0.1 and 0 repeats:\n",
      "F1: 0.42986425339366513\n",
      "sample with fraction of 0.1 and 2 repeats:\n",
      "F1: 0.3788300835654596\n",
      "sample with fraction of 0.1 and 3 repeats:\n",
      "F1: 0.36055603822762816\n",
      "sample with fraction of 0.1 and 4 repeats:\n",
      "F1: 0.3552911177220569\n",
      "sample with fraction of 0.1 and 5 repeats:\n",
      "F1: 0.3497907949790795\n",
      "sample with fraction of 0.2 and 0 repeats:\n",
      "F1: 0.4791344667697063\n",
      "sample with fraction of 0.2 and 2 repeats:\n",
      "F1: 0.42626599888703387\n",
      "sample with fraction of 0.2 and 3 repeats:\n",
      "F1: 0.3946850393700787\n",
      "sample with fraction of 0.2 and 4 repeats:\n",
      "F1: 0.3768913342503439\n",
      "sample with fraction of 0.2 and 5 repeats:\n",
      "F1: 0.3671840354767184\n",
      "sample with fraction of 0.3 and 0 repeats:\n",
      "F1: 0.4985835694050992\n",
      "sample with fraction of 0.3 and 2 repeats:\n",
      "F1: 0.46053489889106325\n",
      "sample with fraction of 0.3 and 3 repeats:\n",
      "F1: 0.4220489977728285\n",
      "sample with fraction of 0.3 and 4 repeats:\n",
      "F1: 0.40224032586558045\n",
      "sample with fraction of 0.3 and 5 repeats:\n",
      "F1: 0.3843586075345732\n",
      "sample with fraction of 0.4 and 0 repeats:\n",
      "F1: 0.5054229934924078\n",
      "sample with fraction of 0.4 and 2 repeats:\n",
      "F1: 0.4838212634822804\n",
      "sample with fraction of 0.4 and 3 repeats:\n",
      "F1: 0.4506517690875234\n",
      "sample with fraction of 0.4 and 4 repeats:\n",
      "F1: 0.4230982787340366\n",
      "sample with fraction of 0.4 and 5 repeats:\n",
      "F1: 0.40641158221303\n",
      "sample with fraction of 0.5 and 0 repeats:\n",
      "F1: 0.4664224664224664\n",
      "sample with fraction of 0.5 and 2 repeats:\n",
      "F1: 0.4893071000855432\n",
      "sample with fraction of 0.5 and 3 repeats:\n",
      "F1: 0.47245179063360887\n",
      "sample with fraction of 0.5 and 4 repeats:\n",
      "F1: 0.4406983744732089\n",
      "sample with fraction of 0.5 and 5 repeats:\n",
      "F1: 0.4220994475138122\n",
      "sample with fraction of 0.6 and 0 repeats:\n",
      "F1: 0.42219215155615697\n",
      "sample with fraction of 0.6 and 2 repeats:\n",
      "F1: 0.4966824644549763\n",
      "sample with fraction of 0.6 and 3 repeats:\n",
      "F1: 0.4850345356868764\n",
      "sample with fraction of 0.6 and 4 repeats:\n",
      "F1: 0.46204188481675396\n",
      "sample with fraction of 0.6 and 5 repeats:\n",
      "F1: 0.4402366863905325\n",
      "sample with fraction of 0.7 and 0 repeats:\n",
      "F1: 0.3883211678832117\n",
      "sample with fraction of 0.7 and 2 repeats:\n",
      "F1: 0.4984740590030519\n",
      "sample with fraction of 0.7 and 3 repeats:\n",
      "F1: 0.4842454394693201\n",
      "sample with fraction of 0.7 and 4 repeats:\n",
      "F1: 0.4765957446808511\n",
      "sample with fraction of 0.7 and 5 repeats:\n",
      "F1: 0.45512010113780027\n",
      "sample with fraction of 0.8 and 0 repeats:\n",
      "F1: 0.365891472868217\n",
      "sample with fraction of 0.8 and 2 repeats:\n",
      "F1: 0.49567099567099565\n",
      "sample with fraction of 0.8 and 3 repeats:\n",
      "F1: 0.48797862867319686\n",
      "sample with fraction of 0.8 and 4 repeats:\n",
      "F1: 0.4847094801223242\n",
      "sample with fraction of 0.8 and 5 repeats:\n",
      "F1: 0.46883468834688347\n",
      "sample with fraction of 0.9 and 0 repeats:\n",
      "F1: 0.36038961038961037\n",
      "sample with fraction of 0.9 and 2 repeats:\n",
      "F1: 0.48056537102473496\n",
      "sample with fraction of 0.9 and 3 repeats:\n",
      "F1: 0.49857819905213263\n",
      "sample with fraction of 0.9 and 4 repeats:\n",
      "F1: 0.4852941176470588\n",
      "sample with fraction of 0.9 and 5 repeats:\n",
      "F1: 0.47988505747126436\n",
      "Max f1 score = 0.5054229934924078 \n",
      "Downsample fraction: 0.4 Upsample repetitions: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5054229934924078"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "find_best_f1(features_train, target_train, model, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see what happens when we combine upsampling/downsampling with the balancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max f1 score = 0.49482758620689654 \n",
      "Downsample fraction: 0.3 Upsample repetitions: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49482758620689654"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear', class_weight='balanced')\n",
    "find_best_f1(features_train, target_train, model, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our results, the max F1 score we can achieve using a logistic regression model is by excluding balancer, using the downsample fraction of **0.4**, and the upsample repetition amount of **0**. Let's now see if we can get better results with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with depth of 2:\n",
      "Max f1 score = 0.541015625 \n",
      "Downsample fraction: 0.0 Upsample repetitions: 3\n",
      "Decision Tree with depth of 3:\n",
      "Max f1 score = 0.558282208588957 \n",
      "Downsample fraction: 0.0 Upsample repetitions: 5\n",
      "Decision Tree with depth of 4:\n",
      "Max f1 score = 0.572987721691678 \n",
      "Downsample fraction: 0.0 Upsample repetitions: 2\n",
      "Decision Tree with depth of 5:\n",
      "Max f1 score = 0.5963791267305644 \n",
      "Downsample fraction: 0.0 Upsample repetitions: 4\n",
      "Decision Tree with depth of 6:\n",
      "Max f1 score = 0.5961538461538461 \n",
      "Downsample fraction: 0.0 Upsample repetitions: 2\n",
      "Decision Tree with depth of 7:\n",
      "Max f1 score = 0.5885486018641812 \n",
      "Downsample fraction: 0.0 Upsample repetitions: 2\n",
      "Decision Tree with depth of 8:\n",
      "Max f1 score = 0.5889724310776943 \n",
      "Downsample fraction: 0.6 Upsample repetitions: 0\n",
      "Decision Tree with depth of 9:\n",
      "Max f1 score = 0.5791411042944784 \n",
      "Downsample fraction: 0.0 Upsample repetitions: 2\n",
      "Best depth = 5, with F1 score of 0.5963791267305644\n"
     ]
    }
   ],
   "source": [
    "best_depth = 0\n",
    "max_f1 = 0\n",
    "for depth in range(2,10):\n",
    "    #create model\n",
    "    dt_model = DecisionTreeClassifier(random_state=123, max_depth=depth)\n",
    "    \n",
    "    print(\"Decision Tree with depth of {}:\".format(depth))\n",
    "    f1 = find_best_f1(features_train, target_train, dt_model, False)\n",
    "    \n",
    "    if f1 > max_f1:\n",
    "        best_depth = depth\n",
    "        max_f1 = f1\n",
    "\n",
    "print(\"Best depth = {}, with F1 score of {}\".format(best_depth, max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with 10 trees:\n",
      "Max f1 score = 0.6086956521739131 \n",
      "Downsample fraction: 0.6 Upsample repetitions: 2\n",
      "Random forest with 20 trees:\n",
      "Max f1 score = 0.6093750000000001 \n",
      "Downsample fraction: 0.6 Upsample repetitions: 0\n",
      "Random forest with 30 trees:\n",
      "Max f1 score = 0.6181818181818183 \n",
      "Downsample fraction: 0.6 Upsample repetitions: 0\n",
      "Random forest with 40 trees:\n",
      "Max f1 score = 0.6234817813765183 \n",
      "Downsample fraction: 0.9 Upsample repetitions: 2\n",
      "Random forest with 50 trees:\n",
      "Max f1 score = 0.6269070735090152 \n",
      "Downsample fraction: 0.7 Upsample repetitions: 0\n",
      "Random forest with 60 trees:\n",
      "Max f1 score = 0.6236263736263736 \n",
      "Downsample fraction: 0.7 Upsample repetitions: 0\n",
      "Random forest with 70 trees:\n",
      "Max f1 score = 0.6217616580310881 \n",
      "Downsample fraction: 0.9 Upsample repetitions: 3\n",
      "Random forest with 80 trees:\n",
      "Max f1 score = 0.6242350061199511 \n",
      "Downsample fraction: 0.5 Upsample repetitions: 0\n",
      "Random forest with 90 trees:\n",
      "Max f1 score = 0.6221142162818954 \n",
      "Downsample fraction: 0.5 Upsample repetitions: 0\n",
      "Best number of trees = 50, with F1 score of 0.6269070735090152\n"
     ]
    }
   ],
   "source": [
    "best_trees = 0\n",
    "max_f1 = 0\n",
    "for trees in range(10,100,10):\n",
    "    #create model\n",
    "    rf_model = RandomForestClassifier(random_state=123,n_estimators=trees)\n",
    "    \n",
    "    print(\"Random forest with {} trees:\".format(trees))\n",
    "    f1 = find_best_f1(features_train, target_train, rf_model, False)\n",
    "    \n",
    "    if f1 > max_f1:\n",
    "        best_trees = trees\n",
    "        max_f1 = f1\n",
    "\n",
    "print(\"Best number of trees = {}, with F1 score of {}\".format(best_trees, max_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the highest F1 score of approximately **0.63** is our **random forest model with 50 trees**, downsampling our negative class observations by 70%. If we want a faster model that still meets our benchmark of .59, we can use a decision tree with a max depth of 5, multiplying our positive class observations by 4. Creating a model using logical regression gives us significantly worse results than with a decision tree or random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Final testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which model to use and the parameters necessary for our model, we'll combine our training and validation sets, balance the classes, create our final model, and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5953389830508474\n",
      "AUC-ROC score: 0.8404547641855216\n"
     ]
    }
   ],
   "source": [
    "#combine our train and validation data sets\n",
    "features = pd.concat([features_train, features_valid])\n",
    "target = pd.concat([target_train, target_valid])\n",
    "\n",
    "#downsample\n",
    "features_downsampled, target_downsampled = create_sample(features, target, .5, 3)\n",
    "\n",
    "#create model\n",
    "model = RandomForestClassifier(random_state=123, n_estimators=50)\n",
    "\n",
    "#train model\n",
    "model.fit(features_downsampled, target_downsampled)\n",
    "\n",
    "#make prediction\n",
    "predicted_test = model.predict(features_test)\n",
    "\n",
    "#get F1 score\n",
    "print(\"F1:\", f1_score(target_test, predicted_test))\n",
    "\n",
    "#get class probability\n",
    "probabilities_valid = model.predict_proba(features_test)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "#measure AUC-ROC\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_valid)\n",
    "\n",
    "print(\"AUC-ROC score:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our F1 score is at about .60, our AUC-ROC score of approximately .84 shows us that our model has a good measure of separability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
